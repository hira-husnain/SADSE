{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ab4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alpha_trans1=np.load('../../../datasets/COIL100/Sb_coil100.npy',allow_pickle=True)\n",
    "alpha_trans1=np.asarray(alpha_trans1).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ddbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "batchSize=720\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from scipy import linalg\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras import backend as B\n",
    "k_cluster = 100\n",
    "from tensorflow.keras.layers import Input,Dense,Lambda,Layer\n",
    "dnet=tf.keras.applications.densenet.DenseNet201(include_top=False,weights=\"imagenet\",input_tensor=None,input_shape=None,pooling=None)\n",
    "from tensorflow.keras.models import Model\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from munkres import Munkres\n",
    "from numpy import save\n",
    "\n",
    "def load_data():\n",
    "    train_set = []\n",
    "    y=np.zeros((7200,1))\n",
    "    i=0\n",
    "    for filename in glob.iglob('../../../datasets/COIL100/coil100images/*.png'):\n",
    "        img = cv2.imread(filename, -1)\n",
    "        #img=cv2.resize(img,dim,interpolation=cv2.INTER_CUBIC)  \n",
    "        train_set.append(img)\n",
    "        parts=os.path.split(filename)[1]\n",
    "        parts=parts.split('_')\n",
    "        parts1=parts[0][3:]\n",
    "        y[i]=parts1\n",
    "        i=i+1\n",
    "        y=y-1\n",
    "    train_set=np.array(train_set)\n",
    "    train_set = train_set.astype('float32') / 255.\n",
    "    train_set, y = shuffle(train_set, y, random_state=10)\n",
    "    print('got train set')\n",
    "    return train_set,y\n",
    "def extract_features(vtr,y):\n",
    "    pca.fit(vtr)\n",
    "    x=pca.transform(vtr)\n",
    "    for i in range(len(x)):\n",
    "        x[i]=x[i]/np.linalg.norm(x[i],2)\n",
    "    y=np.array(y)\n",
    "    y=y.reshape(len(y))\n",
    "    return x,y\n",
    "\n",
    "def eig_vec(x_train):\n",
    "    nev_trans=[]\n",
    "    SortedEigVec=np.zeros((batchSize,batchSize))\n",
    "    for l in range (0,len(x_train),batchSize):\n",
    "        xtr=x_train[l:l+batchSize]\n",
    "        d=(euclidean_distances(xtr,xtr))\n",
    "        W=np.zeros((len(xtr),len(xtr)))\n",
    "        for i in range(len(xtr)):\n",
    "            for j in range (len(xtr)):\n",
    "                if i==j:\n",
    "                    W[i,j]=0\n",
    "                else:\n",
    "                    W[i,j]=np.exp(-(d[i,j]**2)/(2*0.09))\n",
    "        degree=np.diag(sum(W))\n",
    "        L = degree-W\n",
    "        eig_val,eig_vec=np.linalg.eig(L)\n",
    "        dec_eigVal=np.flip(np.sort(eig_val))\n",
    "        indexArr=(np.flip(np.argsort(eig_val)))\n",
    "        for i in range (len(indexArr)):\n",
    "            SortedEigVec[:,i]=eig_vec[:,indexArr[i]]\n",
    "        SortedEigVec=np.array(SortedEigVec)\n",
    "        nev_trans.append(SortedEigVec)\n",
    "    return nev_trans,L,eig_val,eig_vec,dec_eigVal,W\n",
    "\n",
    "def make_cost_matrix(c1, c2):\n",
    "    uc1 = np.unique(c1)\n",
    "    uc2 = np.unique(c2)\n",
    "    l1 = uc1.size\n",
    "    l2 = uc2.size\n",
    "    for a in range (1):\n",
    "        if uc1.shape==uc2.shape:\n",
    "            m = np.ones([l1, l2])\n",
    "            for i in range(l1):\n",
    "                it_i = np.nonzero(c1 == uc1[i])[0]\n",
    "                for j in range(l2):\n",
    "                    it_j = np.nonzero(c2 == uc2[j])[0]\n",
    "                    m_ij = np.intersect1d(it_j, it_i)\n",
    "                    m[i,j] =  -m_ij.size\n",
    "        else:\n",
    "            print('assertion handeled')\n",
    "            return\n",
    "            #break\n",
    "    return m\n",
    "\n",
    "def translate_clustering(clt, mapper):\n",
    "    return np.array([ mapper[i] for i in clt ])\n",
    "\n",
    "def accuracy(cm):\n",
    "    return np.trace(cm, dtype=float) / np.sum(cm)\n",
    "\n",
    "def rectify_label(labels, classes):\n",
    "    num_labels = len(np.unique(classes))\n",
    "    cm = confusion_matrix(classes, labels, labels=range(num_labels)) # gets the confusion matrix\n",
    "    cost_matrix = make_cost_matrix(labels, classes)\n",
    "    if cost_matrix is None:\n",
    "        return (None,None)\n",
    "    else:\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(cost_matrix)\n",
    "        mapper = { old: new for (old, new) in indexes }\n",
    "        new_labels = translate_clustering(labels, mapper)\n",
    "        new_cm = confusion_matrix(classes, new_labels, labels=range(num_labels))\n",
    "        return new_labels,accuracy(new_cm)\n",
    "\n",
    "def arch():\n",
    "    encoding_dim = 710  \n",
    "    input_img = Input(shape=(3000,))\n",
    "    encoded_layer_2 = Dense(2000, activation='tanh')(input_img)\n",
    "    encoded_layer_3 = Dense(1300, activation='tanh')(encoded_layer_2)\n",
    "    encoded_layer_4 = Dense(800, activation='tanh')(encoded_layer_3)\n",
    "    encoded = Dense(encoding_dim, activation='tanh')(encoded_layer_4)\n",
    "    decoded_layer_2 = Dense(800, activation='tanh')(encoded)\n",
    "    decoded_layer_3 = Dense(1300, activation='tanh')(decoded_layer_2)\n",
    "    decoded_layer_4 = Dense(2000, activation='tanh')(decoded_layer_3)\n",
    "    decoded = Dense(3000, activation='tanh')(decoded_layer_4)\n",
    "    encoder=Model(input_img,encoded)\n",
    "    autoencoder=Model(input_img,decoded)\n",
    "    return encoder,encoding_dim,autoencoder\n",
    "  \n",
    "def assign_func(z_test,z_ic):\n",
    "    dst = np.array(np.sum(((z_test-z_ic[0]) **2),axis=1))\n",
    "    for index in range(1, z_ic.shape[0]):\n",
    "        col = np.sum(((z_test- z_ic[index])**2), axis=1)\n",
    "        dst = np.vstack((dst, col))\n",
    "    dst = dst.T      \n",
    "    \n",
    "    mean_dst = np.mean(dst, axis=1)\n",
    "    q = np.maximum(0.0, np.tile(mean_dst, (dst.shape[1], 1)).T - dst)\n",
    "    num_centers = q.shape[1]\n",
    "    weight = 1.0 / (q.sum(axis=0) + 1e-9)\n",
    "    weight *= num_centers / weight.sum()\n",
    "    q = (q ** 2.0) * weight\n",
    "    q = (q.T / (q.sum(axis=1) + B.epsilon())).T\n",
    "    return q\n",
    "\n",
    "\n",
    "def asl():\n",
    "    def myloss(y_true, y_pred):\n",
    "        l1norm= l1norm_func(y_pred,y_true)\n",
    "        l2norm= l2norm_func(y_pred,y_true)\n",
    "        orthonormal= orthonormality_func(y_pred)\n",
    "        structpreserve=structurePreserve_func(y_pred,alpha_trans1[g])\n",
    "        return l1norm+l2norm+structpreserve+orthonormal\n",
    "    return myloss\n",
    "\n",
    "def l1norm_func(z,nt):\n",
    "    q1=0.002*(tf.reduce_sum(tf.abs(tf.subtract(z,nt)),axis=1))\n",
    "    return q1\n",
    "\n",
    "def l2norm_func(z,nt):\n",
    "    q2=0.02*(tf.sqrt(tf.reduce_sum(tf.square(z-nt))))\n",
    "    return q2\n",
    "\n",
    "\n",
    "def structurePreserve_func(z,ta):\n",
    "    tz=tf.transpose(z)\n",
    "    tz=tf.reshape(tz,(encoding_dim,batchSize))\n",
    "    taa=tf.transpose(ta)\n",
    "    taz=tf.matmul(tz,taa)\n",
    "    taz=tf.reshape(taz,(encoding_dim,batchSize))\n",
    "    q4=0.02*(tf.sqrt(tf.reduce_sum(tf.square(tz-taz))))\n",
    "    q4=tf.transpose(q4)\n",
    "    return q4\n",
    "\n",
    "\n",
    "def orthonormality_func(z):\n",
    "    z=[(z[i,:]/(tf.norm(z[i,:],2)))for i in range (batchSize)]\n",
    "    transz=tf.transpose(z)\n",
    "    q3=tf.matmul(transz,z)\n",
    "    I=tf.eye(encoding_dim)\n",
    "    q5=0.02*(tf.sqrt(tf.reduce_sum(tf.square(q3-I))))\n",
    "    return q5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,y=load_data()\n",
    "vtr=dnet.predict(train_set)\n",
    "vtr = vtr.reshape((len(vtr), np.prod(vtr.shape[1:])))\n",
    "x_train,y=extract_features(vtr,y)\n",
    "x_train=x_train.reshape(7200,3000)\n",
    "y=y.reshape(7200,)\n",
    "nev_trans,L,eig_val,eig_vec,dec_eigVal,W=eig_vec(x_train)\n",
    "alpha_trans1=np.load('../../../datasets/COIL100/Sb_coil100.npy',allow_pickle=True)\n",
    "alpha_trans1=np.asarray(alpha_trans1).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766909fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder,encoding_dim,autoencoder=arch()\n",
    "autoencoder.compile(tf.keras.optimizers.Adam(lr=1e-3), loss=tf.keras.losses.MeanSquaredError())\n",
    "autoencoder.fit(x_train,x_train,epochs=100,shuffle=True,batch_size=batchSize)\n",
    "encoder.save_weights('../../../newcheckpoints/nchk_coil100/nE_Coil100.ckpt')\n",
    "\n",
    "print('training from scratch')\n",
    "encoder.load_weights('../../../newcheckpoints/nchk_coil100/nE_Coil100.ckpt')\n",
    "g=0\n",
    "encoder.compile(tf.keras.optimizers.Adadelta(lr=1e-3, rho=0.95, decay=1e-4), loss=asl())\n",
    "\n",
    "acc=0\n",
    "for t in range (1,500,1):\n",
    "    g=0\n",
    "    for l in range (0,len(x_train),batchSize):\n",
    "        encoder.fit(x_train[l:l+batchSize],nev_trans[g][:,batchSize-encoding_dim-1:batchSize-1],epochs=1,shuffle=False,batch_size=batchSize)\n",
    "        g=g+1\n",
    "    z_test=encoder.predict(x_train)\n",
    "    k_means = KMeans(n_clusters=k_cluster, init='k-means++')\n",
    "    ic= k_means.fit(z_test).cluster_centers_ \n",
    "    assignment = assign_func(z_test, ic)\n",
    "    predictedlabels = np.argmax(assignment, axis=1)\n",
    "    rectified_label, num=rectify_label(predictedlabels, y)\n",
    "    if rectified_label is not None:\n",
    "        print('acc at',t, 'epoch is:',acc)\n",
    "        if acc<num:\n",
    "            acc=num\n",
    "            encoder.save_weights('../../../newcheckpoints/nchk_coil100/nsdsc_Coil100.ckpt')\n",
    "            np.save('../../../newcheckpoints/nchk_coil100/nsdsc_icCoil100.npy',ic)\n",
    "            nmi=sklearn.metrics.normalized_mutual_info_score(y, predictedlabels)\n",
    "            f1=sklearn.metrics.f1_score(y, rectified_label,average='micro')\n",
    "            prec=sklearn.metrics.precision_score(y, rectified_label,average='weighted')\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008126fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test=encoder.predict(x_test)\n",
    "assignment = assign_func(z_test, ic)\n",
    "predictedlabels = np.argmax(assignment, axis=1)\n",
    "rectified_label, acc=rectify_label(predictedlabels, y_test)\n",
    "nmi=sklearn.metrics.normalized_mutual_info_score(y_test, predictedlabels)\n",
    "f1=sklearn.metrics.f1_score(y_test, predictedlabels,average='micro')\n",
    "prec=sklearn.metrics.precision_score(y_test, predictedlabels,average='weighted')\n",
    "print(acc,nmi,f1,prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75857e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc,nmi,f1,prec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
