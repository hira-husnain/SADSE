{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import scipy.sparse as sparse\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, hid_dims, out_dims, kaiming_init=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.hid_dims = hid_dims\n",
    "        self.output_dims = out_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.input_dims, self.hid_dims[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        for i in range(len(hid_dims) - 1):\n",
    "            self.layers.append(nn.Linear(self.hid_dims[i], self.hid_dims[i + 1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.out_layer = nn.Linear(self.hid_dims[-1], self.output_dims)\n",
    "        if kaiming_init:\n",
    "            self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.kaiming_uniform_(layer.weight)\n",
    "                init.zeros_(layer.bias)\n",
    "        init.xavier_uniform_(self.out_layer.weight)\n",
    "        init.zeros_(self.out_layer.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "        h = self.out_layer(h)\n",
    "        h = torch.tanh_(h)\n",
    "        return h\n",
    "\n",
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, hid_dims, out_dims, kaiming_init=True):\n",
    "        super(SENet, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.hid_dims = hid_dims\n",
    "        self.out_dims = out_dims\n",
    "        self.kaiming_init = kaiming_init\n",
    "        self.net_q = MLP(input_dims=self.input_dims,\n",
    "                         hid_dims=self.hid_dims,\n",
    "                         out_dims=self.out_dims,\n",
    "                         kaiming_init=self.kaiming_init)\n",
    "\n",
    "        self.net_k = MLP(input_dims=self.input_dims,\n",
    "                         hid_dims=self.hid_dims,\n",
    "                         out_dims=self.out_dims,\n",
    "                         kaiming_init=self.kaiming_init)\n",
    "\n",
    "    def query_embedding(self, queries):\n",
    "        q_emb = self.net_q(queries)\n",
    "        return q_emb\n",
    "    \n",
    "    def key_embedding(self, keys):\n",
    "        k_emb = self.net_k(keys)\n",
    "        return k_emb\n",
    "\n",
    "    def get_coeff(self, q_emb, k_emb):\n",
    "        c=q_emb.mm(k_emb.t())\n",
    "        return c\n",
    "\n",
    "    def forward(self, queries, keys):\n",
    "        q = self.query_embedding(queries)\n",
    "        k = self.key_embedding(keys)\n",
    "        out = self.get_coeff(q_emb=q, k_emb=k)\n",
    "        return out\n",
    "\n",
    "\n",
    "def regularizer(c, lmbd=1.0):\n",
    "    return lmbd * torch.abs(c).sum() + (1.0 - lmbd) / 2.0 * torch.pow(c, 2).sum()\n",
    "\n",
    "\n",
    "def get_sparse_rep(senet, data, batch_size=10, chunk_size=100, non_zeros=1000):\n",
    "\n",
    "    N, D = data.shape\n",
    "    non_zeros = min(N, non_zeros)\n",
    "    C = torch.empty([batch_size, N])\n",
    "    if (N % batch_size != 0):\n",
    "        raise Exception(\"batch_size should be a factor of dataset size.\")\n",
    "    if (N % chunk_size != 0):\n",
    "        raise Exception(\"chunk_size should be a factor of dataset size.\")\n",
    "\n",
    "    val = []\n",
    "    indicies = []\n",
    "    with torch.no_grad():\n",
    "        senet.eval()\n",
    "        for i in range(data.shape[0] // batch_size):\n",
    "            chunk = data[i * batch_size:(i + 1) * batch_size].cuda()\n",
    "            q = senet.query_embedding(chunk)\n",
    "            for j in range(data.shape[0] // chunk_size):\n",
    "                chunk_samples = data[j * chunk_size: (j + 1) * chunk_size].cuda()\n",
    "                k = senet.key_embedding(chunk_samples)\n",
    "                temp = senet.get_coeff(q, k)\n",
    "                C[:, j * chunk_size:(j + 1) * chunk_size] = temp.cpu()\n",
    "             \n",
    "\n",
    "            rows = list(range(batch_size))\n",
    "            cols = [j + i * batch_size for j in rows]\n",
    "            C[rows, cols] = 0.0\n",
    "            _, index = torch.topk(torch.abs(C), dim=1, k=non_zeros)\n",
    "            \n",
    "            val.append(C.gather(1, index).reshape([-1]).cpu().data.numpy())\n",
    "            index = index.reshape([-1]).cpu().data.numpy()\n",
    "            indicies.append(index)\n",
    "\n",
    "    val = np.concatenate(val, axis=0)\n",
    "    indicies = np.concatenate(indicies, axis=0)\n",
    "    indptr = [non_zeros * i for i in range(N + 1)]\n",
    "    \n",
    "    C_sparse = sparse.csr_matrix((val, indicies, indptr), shape=[N, N])\n",
    "    return C_sparse\n",
    "\n",
    "\n",
    "def get_knn_Aff(C_sparse_normalized, k=3, mode='symmetric'):\n",
    "    C_knn = kneighbors_graph(C_sparse_normalized, k, mode='connectivity', include_self=False, n_jobs=10)\n",
    "    csn_array.append(C_knn.toarray())\n",
    "    return csn_array\n",
    "\n",
    "\n",
    "def evaluate(csn_array,senet, data, labels, num_subspaces, spectral_dim, non_zeros=1000, n_neighbors=3,\n",
    "             batch_size=10000, chunk_size=10000, affinity='nearest_neighbor', knn_mode='symmetric'):\n",
    "    C_sparse = get_sparse_rep(senet=senet, data=data, batch_size=batch_size,\n",
    "                              chunk_size=chunk_size, non_zeros=non_zeros)\n",
    "    C_sparse_normalized = normalize(C_sparse).astype(np.float32)\n",
    "    csn_array = get_knn_Aff(C_sparse_normalized, k=n_neighbors, mode=knn_mode)\n",
    "    return csn_array\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default=\"ORL\")\n",
    "    parser.add_argument('--num_subspaces', type=int, default=40)\n",
    "    parser.add_argument('--gamma', type=float, default=200.0)\n",
    "    parser.add_argument('--lmbd', type=float, default=0.9)\n",
    "    parser.add_argument('--hid_dims', type=int, default=[1024, 1024, 1024])\n",
    "    parser.add_argument('--out_dims', type=int, default=1024)\n",
    "    parser.add_argument('--total_iters', type=int, default=100000)\n",
    "    parser.add_argument('--save_iters', type=int, default=200000)\n",
    "    parser.add_argument('--eval_iters', type=int, default=200000)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--lr_min', type=float, default=0.0)\n",
    "    parser.add_argument('--batch_size', type=int, default=400)\n",
    "    parser.add_argument('--chunk_size', type=int, default=40000)\n",
    "    parser.add_argument('--non_zeros', type=int, default=40000)\n",
    "    parser.add_argument('--n_neighbors', type=int, default=3)\n",
    "    parser.add_argument('--spectral_dim', type=int, default=15)\n",
    "    parser.add_argument('--affinity', type=str, default=\"nearest_neighbor\")\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    fit_msg = \"Experiments on {}, numpy_seed=0, total_iters=100000, lambda=0.9, gamma=200.0\".format(args.dataset, args.seed)\n",
    "    print(fit_msg)\n",
    "\n",
    "    same_seeds(args.seed)\n",
    "    tic = time.time()\n",
    "\n",
    "    full_samples = np.load( '../../../datasets/ORL/orltraindataforSb.npy')\n",
    "    full_labels = np.load( '../../../datasets/ORL/orltrainlabelsforSb.npy')\n",
    "    full_labels = full_labels.reshape(len(full_labels), )\n",
    "\n",
    "\n",
    "    \n",
    "    full_labels = full_labels - np.min(full_labels) \n",
    "\n",
    "    result = open('{}/results.csv'.format(folder), 'w')\n",
    "    writer = csv.writer(result)\n",
    "    writer.writerow([\"N\", \"ACC\", \"NMI\", \"ARI\"])\n",
    "\n",
    "    global_steps = 0\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"SDSC At start torch.cuda.memory_allocated for orl train: %fGB\" % (torch.cuda.memory_allocated(0) / 1024 / 1024 / 1024))\n",
    "    print(\"SDSC At start torch.cuda.memory_reserved for orl train: %fGB\" % (torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024))\n",
    "    print(\"SDSC At start torch.cuda.max_memory_reserved for orl train: %fGB\" % (torch.cuda.max_memory_reserved(0) / 1024 / 1024 / 1024))\n",
    "    for N in [len(full_samples)]:\n",
    "        sampled_idx = np.random.choice(full_samples.shape[0], N, replace=False)\n",
    "        samples, labels = full_samples[sampled_idx], full_labels[sampled_idx]\n",
    "\n",
    "        block_size = min(N, 10000)\n",
    "      \n",
    "        with open('{}/{}_samples_{}.pkl'.format(folder, args.dataset, N), 'wb') as f:\n",
    "            pickle.dump(samples, f)\n",
    "        with open('{}/{}_labels_{}.pkl'.format(folder, args.dataset, N), 'wb') as f:\n",
    "            pickle.dump(labels, f)\n",
    "\n",
    "        all_samples, ambient_dim = samples.shape[0], samples.shape[1]\n",
    "\n",
    "        data = torch.from_numpy(samples).float()\n",
    "        data = utils.p_normalize(data)\n",
    "\n",
    "        n_iter_per_epoch = samples.shape[0] // args.batch_size\n",
    "        n_step_per_iter = round(all_samples // block_size)\n",
    "        n_epochs = args.total_iters // n_iter_per_epoch\n",
    "        \n",
    "        senet = SENet(ambient_dim, args.hid_dims, args.out_dims, kaiming_init=True).cuda()\n",
    "        optimizer = optim.Adam(senet.parameters(), lr=args.lr)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=args.lr_min)\n",
    "\n",
    "        n_iters = 0\n",
    "        pbar = tqdm(range(n_epochs), ncols=120)\n",
    "\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            randidx = torch.randperm(data.shape[0])\n",
    "\n",
    "            for i in range(n_iter_per_epoch):\n",
    "                senet.train()\n",
    "\n",
    "                batch_idx = randidx[i * args.batch_size : (i + 1) * args.batch_size]\n",
    "                batch = data[batch_idx].cuda()\n",
    "                batch=data[i*args.batch_size:i*args.batch_size+args.batch_size].cuda()\n",
    "\n",
    "                q_batch = senet.query_embedding(batch)\n",
    "                k_batch = senet.key_embedding(batch)\n",
    "\n",
    "                rec_batch = torch.zeros_like(batch).cuda()\n",
    "                reg = torch.zeros([1]).cuda()\n",
    "                for j in range(n_step_per_iter):\n",
    "                    block = data[j * block_size: (j + 1) * block_size].cuda()\n",
    "                    k_block = senet.key_embedding(block)\n",
    "                    c = senet.get_coeff(q_batch, k_block)\n",
    "                    rec_batch = rec_batch + c.mm(block)\n",
    "                    reg = reg + regularizer(c, args.lmbd)\n",
    "\n",
    "                #diag_c = senet.thres((q_batch * k_batch).sum(dim=1, keepdim=True)) * senet.shrink\n",
    "                diag_c = (q_batch * k_batch).sum(dim=1, keepdim=True)\n",
    "                rec_batch = rec_batch - diag_c * batch\n",
    "                reg = reg - regularizer(diag_c, args.lmbd)\n",
    "\n",
    "                rec_loss = torch.sum(torch.pow(batch - rec_batch, 2))\n",
    "                loss = (0.5 * args.gamma * rec_loss + reg) / args.batch_size\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(senet.parameters(), 0.001)\n",
    "                optimizer.step()\n",
    "\n",
    "                global_steps += 1\n",
    "                n_iters += 1\n",
    "\n",
    "\n",
    "                if n_iters % args.save_iters == 0:\n",
    "                    with open('{}/SENet_{}_N{:d}_iter{:d}.pth.tar'.format(folder, args.dataset, N, n_iters), 'wb') as f:\n",
    "                        torch.save(senet.state_dict(), f)\n",
    "                    print(\"Model Saved.\")\n",
    "\n",
    "                if n_iters % args.eval_iters == 0:\n",
    "                    print(\"Evaluating on sampled data...\")\n",
    "                    acc, nmi, ari,csn_array = evaluate(csn_array,senet, data=data, labels=labels, num_subspaces=args.num_subspaces, affinity=args.affinity,spectral_dim=args.spectral_dim, non_zeros=args.non_zeros, n_neighbors=args.n_neighbors,batch_size=block_size, chunk_size=block_size,knn_mode='symmetric')\n",
    "                    print(\"ACC-{:.6f}, NMI-{:.6f}, ARI-{:.6f}\".format(acc, nmi, ari))\n",
    "\n",
    "            pbar.set_postfix(loss=\"{:3.4f}\".format(loss.item()),\n",
    "                             rec_loss=\"{:3.4f}\".format(rec_loss.item() / args.batch_size),\n",
    "                             reg=\"{:3.4f}\".format(reg.item() / args.batch_size))\n",
    "            scheduler.step()\n",
    "        end = time.time()\n",
    "        print('SDSC time to converge C on orl train ', end - start)\n",
    "\n",
    "        print(\"SDSC after convergence torch.cuda.memory_allocated for orl train: %fGB\" % (torch.cuda.memory_allocated(0) / 1024 / 1024 / 1024))\n",
    "        print(\"SDSC After convergence torch.cuda.memory_reserved for orl train: %fGB\" % (torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024))\n",
    "        print(\"SDSC After convergence torch.cuda.max_memory_reserved for orl train: %fGB\" % (torch.cuda.max_memory_reserved(0) / 1024 / 1024 / 1024))\n",
    "\n",
    "        print(\"Evaluating on eyaleb full....\".format(args.dataset))\n",
    "        full_data = torch.from_numpy(full_samples).float()\n",
    "        full_data = utils.p_normalize(full_data)\n",
    "        csn_array = []\n",
    "        d = []\n",
    "        l = []\n",
    "        start = time.time()\n",
    "        for i in range(int(len(full_data) / args.batch_size)):\n",
    "            databatch = full_data[i * args.batch_size:i * args.batch_size + args.batch_size]\n",
    "            labelbatch = full_labels[i * args.batch_size:i * args.batch_size + args.batch_size]\n",
    "            d.append(databatch.numpy())\n",
    "            l.append(labelbatch)\n",
    "            acc, nmi, ari, csn_array = evaluate(csn_array, senet, data=databatch, labels=labelbatch,\n",
    "                                            num_subspaces=args.num_subspaces, affinity=args.affinity,\n",
    "                                            spectral_dim=args.spectral_dim, non_zeros=args.non_zeros,\n",
    "                                            n_neighbors=args.n_neighbors, batch_size=args.batch_size,\n",
    "                                            chunk_size=args.batch_size, knn_mode='symmetric')\n",
    "        print(\"N-{:d}: ACC-{:.6f}, NMI-{:.6f}, ARI-{:.6f}\".format(N, acc, nmi, ari))\n",
    "        end = time.time()\n",
    "        print('time to compute C on orl train is : ', end - start)\n",
    "        print(np.array(csn_array).shape)\n",
    "\n",
    "        np.save( '../../../datasets/ORL/nSb_orltrain.npy',csn_array)\n",
    "        np.save('../../../datasets/ORL/nd_orltrain.npy', np.array(d))\n",
    "        np.save('../../../datasets/ORL/nl_orltrain.npy', np.array(l))\n",
    "\n",
    "        print(\"SDSC At end torch.cuda.memory_allocated for orl train: %fGB\" % (torch.cuda.memory_allocated(0) / 1024 / 1024 / 1024))\n",
    "        print(\"SDSC At end torch.cuda.memory_reserved for orl train: %fGB\" % (torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024))\n",
    "        print(\"SDSC At end torch.cuda.max_memory_reserved for orl train: %fGB\" % (torch.cuda.max_memory_reserved(0) / 1024 / 1024 / 1024))\n",
    "\n",
    "        writer.writerow([N, acc, nmi, ari])\n",
    "        result.flush()\n",
    "\n",
    "        with open('{}/SENet_{}_N{:d}.pth.tar'.format(folder, args.dataset, N), 'wb') as f:\n",
    "            torch.save(senet.state_dict(), f)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    result.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
