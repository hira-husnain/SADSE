{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "batchSize=400\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from scipy import linalg\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras import backend as B\n",
    "k_cluster = 40\n",
    "from tensorflow.keras.layers import Input,Dense,Lambda,Layer\n",
    "dnet=tf.keras.applications.densenet.DenseNet121(include_top=False,weights=\"imagenet\",input_tensor=None,input_shape=None,pooling=None)\n",
    "from tensorflow.keras.models import Model\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from munkres import Munkres\n",
    "import cv2\n",
    "from numpy import save\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_set = []\n",
    "    y= []\n",
    "    #dim=(48,48)\n",
    "    counter_i = 0\n",
    "    for filename in glob.iglob('../../../datasets/ORL/orl-dataset/*/*.pgm'):\n",
    "        img = cv2.imread(filename, -1)\n",
    "        #img=cv2.resize(img,dim,interpolation=cv2.INTER_LINEAR)  \n",
    "        train_set.append(img)\n",
    "        parts=os.path.split(filename)[0]\n",
    "        parts=parts.split('/')[10]\n",
    "        parts=parts.split('s')[1]\n",
    "        y.append(parts)\n",
    "        print(counter_i)\n",
    "        counter_i += 1\n",
    "    y=np.int32(y)\n",
    "    y=y-1\n",
    "    train_set, y = shuffle(train_set, y, random_state=42)\n",
    "    return train_set,y\n",
    "\n",
    "def extract_features(vtr):\n",
    "    for i in range(len(vtr)):\n",
    "        vtr[i]=vtr[i]/np.linalg.norm(vtr[i],2)\n",
    "    pca.fit(vtr)\n",
    "    x=pca.transform(vtr)\n",
    "    return x\n",
    "\n",
    "def eig_vec(x_train):\n",
    "    nev_trans=[]\n",
    "    SortedEigVec=np.zeros((batchSize,batchSize))\n",
    "    for l in range (0,len(x_train),batchSize):\n",
    "        xtr=x_train[l:l+batchSize]\n",
    "        d=(euclidean_distances(xtr,xtr))\n",
    "        W=np.zeros((len(xtr),len(xtr)))\n",
    "        for i in range(len(xtr)):\n",
    "            for j in range (len(xtr)):\n",
    "                if i==j:\n",
    "                    W[i,j]=0\n",
    "                else:\n",
    "                    W[i,j]=np.exp(-(d[i,j]**2)/(2*0.09))\n",
    "        degree=np.diag(sum(W))\n",
    "        L = degree-W\n",
    "        eig_val,eig_vec=np.linalg.eig(L)\n",
    "        dec_eigVal=np.flip(np.sort(eig_val))\n",
    "        indexArr=(np.flip(np.argsort(eig_val)))\n",
    "        for i in range (len(indexArr)):\n",
    "            SortedEigVec[:,i]=eig_vec[:,indexArr[i]]\n",
    "        SortedEigVec=np.array(SortedEigVec)\n",
    "        nev_trans.append(SortedEigVec)\n",
    "    return nev_trans,L,eig_val,eig_vec,dec_eigVal,W\n",
    "\n",
    "def make_cost_matrix(c1, c2):\n",
    "    uc1 = np.unique(c1)\n",
    "    uc2 = np.unique(c2)\n",
    "    l1 = uc1.size\n",
    "    l2 = uc2.size\n",
    "    for a in range (1):\n",
    "        if uc1.shape==uc2.shape:\n",
    "            m = np.ones([l1, l2])\n",
    "            for i in range(l1):\n",
    "                it_i = np.nonzero(c1 == uc1[i])[0]\n",
    "                for j in range(l2):\n",
    "                    it_j = np.nonzero(c2 == uc2[j])[0]\n",
    "                    m_ij = np.intersect1d(it_j, it_i)\n",
    "                    m[i,j] =  -m_ij.size\n",
    "        else:\n",
    "            print('assertion handeled')\n",
    "            return\n",
    "            #break\n",
    "    return m\n",
    "\n",
    "def translate_clustering(clt, mapper):\n",
    "    return np.array([ mapper[i] for i in clt ])\n",
    "\n",
    "def accuracy(cm):\n",
    "    return np.trace(cm, dtype=float) / np.sum(cm)\n",
    "\n",
    "def rectify_label(labels, classes):\n",
    "    num_labels = len(np.unique(classes))\n",
    "    cm = confusion_matrix(classes, labels, labels=range(num_labels)) # gets the confusion matrix\n",
    "    cost_matrix = make_cost_matrix(labels, classes)\n",
    "    if cost_matrix is None:\n",
    "        return (None,None)\n",
    "    else:\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(cost_matrix)\n",
    "        mapper = { old: new for (old, new) in indexes }\n",
    "        new_labels = translate_clustering(labels, mapper)\n",
    "        new_cm = confusion_matrix(classes, new_labels, labels=range(num_labels))\n",
    "        return new_labels,accuracy(new_cm)\n",
    "    \n",
    "def arch():\n",
    "    encoding_dim = 390  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "    input_img = Input(shape=(400,))\n",
    "    encoded_layer_2 = Dense(397, activation='tanh')(input_img)\n",
    "    encoded_layer_3 = Dense(395, activation='tanh')(encoded_layer_2)\n",
    "    encoded_layer_4 = Dense(393, activation='tanh')(encoded_layer_3)\n",
    "    encoded = Dense(encoding_dim, activation='tanh')(encoded_layer_4)\n",
    "    decoded_layer_2 = Dense(393, activation='tanh')(encoded)\n",
    "    decoded_layer_3 = Dense(395, activation='tanh')(decoded_layer_2)\n",
    "    decoded_layer_4 = Dense(397, activation='tanh')(decoded_layer_3)\n",
    "    decoded = Dense(400, activation='tanh')(decoded_layer_4)\n",
    "    encoder=Model(input_img,encoded)\n",
    "    autoencoder=Model(input_img,decoded)\n",
    "    return encoder,encoding_dim,autoencoder\n",
    "\n",
    "\n",
    "def assign_func(z_test,z_ic):\n",
    "    dst = np.array(np.sum(((z_test-z_ic[0]) **2),axis=1))\n",
    "    for index in range(1, z_ic.shape[0]):\n",
    "        col = np.sum(((z_test- z_ic[index])**2), axis=1)\n",
    "        dst = np.vstack((dst, col))\n",
    "    dst = dst.T      \n",
    "    \n",
    "    mean_dst = np.mean(dst, axis=1)\n",
    "    q = np.maximum(0.0, np.tile(mean_dst, (dst.shape[1], 1)).T - dst)\n",
    "    #print(q.shape)\n",
    "    num_centers = q.shape[1]\n",
    "    weight = 1.0 / (q.sum(axis=0) + 1e-9)\n",
    "    weight *= num_centers / weight.sum()\n",
    "    q = (q ** 2.0) * weight\n",
    "    q = (q.T / (q.sum(axis=1) + B.epsilon())).T\n",
    "    return q\n",
    "\n",
    "def asl():\n",
    "    def myloss(y_true, y_pred):\n",
    "        l1norm= l1norm_func(y_pred,y_true)\n",
    "        l2norm= l2norm_func(y_pred,y_true)\n",
    "        orthonormal= orthonormality_func(y_pred)\n",
    "        structpreserve=structurePreserve_func(y_pred,alpha_trans1[g][:,:])\n",
    "        return orthonormal+l1norm+l2norm+structpreserve\n",
    "    return myloss\n",
    "\n",
    "def l1norm_func(z,nt):\n",
    "    q1=0.002*(tf.reduce_sum(tf.abs(tf.subtract(z,nt)),axis=1))\n",
    "    return q1\n",
    "\n",
    "def l2norm_func(z,nt):\n",
    "    q2=0.02*(tf.sqrt(tf.reduce_sum(tf.square(z-nt),axis=1)))\n",
    "    return q2\n",
    "\n",
    "def structurePreserve_func(z,ta):\n",
    "    tz=tf.transpose(z)\n",
    "    tz=tf.reshape(tz,(encoding_dim,batchSize))\n",
    "    taa=tf.transpose(ta)\n",
    "    taz=tf.matmul(tz,taa)\n",
    "    taz=tf.reshape(taz,(encoding_dim,batchSize))\n",
    "    q4=0.02*(tf.sqrt(tf.reduce_sum(tf.square(tz-taz))))\n",
    "    q4=tf.transpose(q4)\n",
    "    return q4\n",
    "\n",
    "\n",
    "def orthonormality_func(z):\n",
    "    z=[(z[i,:]/(tf.norm(z[i,:],2)))for i in range (batchSize)]\n",
    "    transz=tf.transpose(z)\n",
    "    q3=tf.tensordot(transz,z,axes=1)\n",
    "    I=tf.eye(encoding_dim)\n",
    "    q5=0.002*(tf.sqrt(tf.reduce_sum(tf.square(q3-I))))\n",
    "    return q5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c62f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,y1=load_data()\n",
    "train_set=np.array(train_set)\n",
    "y1=np.array(y1)\n",
    "train_set=np.stack((train_set,)*3, axis=-1)\n",
    "train_set = train_set.astype('float32') / 255.\n",
    "vtr=dnet.predict(train_set)\n",
    "vtr = vtr.reshape((len(vtr), np.prod(vtr.shape[1:])))\n",
    "x,y=extract_features(vtr,y)\n",
    "x_train=x\n",
    "y=y1\n",
    "\n",
    "x1=x_train[:216,:]\n",
    "y1=y[:216,]\n",
    "x2=x_train[296:,:]\n",
    "y2=y[296:,]\n",
    "x300=np.concatenate((x1,x2),axis=0)\n",
    "y300=np.concatenate((y1,y2),axis=0)\n",
    "idx = np.random.choice(np.arange(len(x300)), 80, replace=False)\n",
    "x_sample = x300[idx]\n",
    "y_sample = y300[idx]\n",
    "\n",
    "xtr=np.concatenate((x_sample,x300),axis=0)\n",
    "ytr=np.concatenate((y_sample,y300),axis=0)\n",
    "print(xtr.shape,ytr.shape,np.unique(ytr))\n",
    "\n",
    "# np.save('orltraindataforSb.npy',xtr)\n",
    "# np.save('orltrainlabelsforSb.npy',ytr)\n",
    "\n",
    "x_test=x_train[216:296,:]\n",
    "y_test=y[216:296,]\n",
    "# np.save('orltestdata.npy',x_test)\n",
    "# np.save('orltestlabels.npy',y_test)\n",
    "# np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "nev_trans,L,eig_val,eig_vec,dec_eigVal,W=eig_vec(x_train)\n",
    "alpha_trans1=np.load('../../../datasets/ORL/Sb_orltrain.npy',allow_pickle=True)\n",
    "alpha_trans1=np.asarray(alpha_trans1).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d741720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder,encoding_dim,autoencoder=arch()\n",
    "autoencoder.compile(tf.keras.optimizers.Adam(lr=1e-3), loss=tf.keras.losses.MeanSquaredError())\n",
    "autoencoder.fit(x_train,x_train,epochs=100,shuffle=True,batch_size=batchSize)\n",
    "encoder.save_weights('../../../new_checkpoints/nchk_orl/nE_orltrain.ckpt')\n",
    "\n",
    "print('training from scratch')\n",
    "encoder.load_weights('../../../new_checkpoints/nchk_orl/nE_orltrain.ckpt')\n",
    "g=0\n",
    "encoder.compile(tf.keras.optimizers.Adadelta(lr=1e-3, rho=0.95, decay=1e-4), loss=asl())\n",
    "\n",
    "acc=0\n",
    "for t in range (1,4000,1):\n",
    "    g=0\n",
    "    for l in range (0,len(x_train),batchSize):\n",
    "        encoder.fit(x_train[l:l+batchSize],nev_trans[g][:,batchSize-encoding_dim-1:batchSize-1],epochs=1,shuffle=False,batch_size=batchSize)\n",
    "        g=g+1\n",
    "    z_test=encoder.predict(x_train)\n",
    "    k_means = KMeans(n_clusters=k_cluster, init='k-means++')\n",
    "    ic= k_means.fit(z_test).cluster_centers_ \n",
    "    assignment = assign_func(z_test, ic)\n",
    "    predictedlabels = np.argmax(assignment, axis=1)\n",
    "    rectified_label, num=rectify_label(predictedlabels, y)\n",
    "    if rectified_label is not None:\n",
    "        print('acc at',t, 'epoch is:',acc)\n",
    "        if acc<num:\n",
    "            acc=num\n",
    "            encoder.save_weights('../../../new_checkpoints/nchk_orl/nsdsc_orltrain.ckpt')\n",
    "            np.save('../../../new_checkpoints/nchk_orl/nsdsc_icorltrain.npy',ic)\n",
    "            nmi=sklearn.metrics.normalized_mutual_info_score(y, rectified_label)\n",
    "            f1=sklearn.metrics.f1_score(y, rectified_label,average='micro')\n",
    "            prec=sklearn.metrics.precision_score(y, rectified_label,average='weighted')\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b015d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc,nmi,f1,prec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
