{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1059039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train=np.load('../../../datasets/MNIST/d_mnist40k.npy')\n",
    "x_train=x_train.reshape(x_train.shape[0]*x_train.shape[1],2000)\n",
    "y=np.load('../../../datasets/MNIST/l_mnist40k.npy')\n",
    "y=y.reshape(y.shape[0]*y.shape[1],)\n",
    "alpha_trans1=np.load('../../../datasets/MNIST/Sb_mnist40k.npy',allow_pickle=True)\n",
    "alpha_trans1=np.asarray(alpha_trans1).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31383320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "batchSize=500\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from scipy import linalg\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras import backend as B\n",
    "k_cluster = 10\n",
    "from tensorflow.keras.layers import Input,Dense,Lambda,Layer\n",
    "from tensorflow.keras.models import Model\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from munkres import Munkres\n",
    "\n",
    "\n",
    "def make_cost_matrix(c1, c2):\n",
    "    uc1 = np.unique(c1)\n",
    "    uc2 = np.unique(c2)\n",
    "    l1 = uc1.size\n",
    "    l2 = uc2.size\n",
    "    #print('uc1 shape is',l1)\n",
    "    #print('uc2 shape is',l2)\n",
    "    \n",
    "    ## if 10==10 return true else return false\n",
    "    assert(l1 == l2 and np.all(uc1 == uc2))\n",
    "    m = np.ones([l1, l2])\n",
    "    for i in range(l1):\n",
    "        it_i = np.nonzero(c1 == uc1[i])[0]\n",
    "        for j in range(l2):\n",
    "            it_j = np.nonzero(c2 == uc2[j])[0]\n",
    "            m_ij = np.intersect1d(it_j, it_i)\n",
    "            m[i,j] =  -m_ij.size\n",
    "    return m\n",
    "\n",
    "def translate_clustering(clt, mapper):\n",
    "    return np.array([ mapper[i] for i in clt ])\n",
    "\n",
    "def accuracy(cm):\n",
    "    return np.trace(cm, dtype=float) / np.sum(cm)\n",
    "\n",
    "def rectify_label(labels, classes):\n",
    "    num_labels = len(np.unique(classes))\n",
    "    cm = confusion_matrix(classes, labels, labels=range(num_labels)) # gets the confusion matrix\n",
    "    cost_matrix = make_cost_matrix(labels, classes) \n",
    "    m = Munkres()\n",
    "    indexes = m.compute(cost_matrix)\n",
    "    mapper = { old: new for (old, new) in indexes }\n",
    "    new_labels = translate_clustering(labels, mapper)\n",
    "    new_cm = confusion_matrix(classes, new_labels, labels=range(num_labels))\n",
    "    return new_labels,accuracy(new_cm)\n",
    "\n",
    "\n",
    "def arch():\n",
    "    encoding_dim = 490 \n",
    "    input_img = Input(shape=(2000,))\n",
    "    encoded_layer_2 = Dense(1500, activation='tanh')(input_img)\n",
    "    encoded_layer_3 = Dense(1000, activation='tanh')(encoded_layer_2)\n",
    "    encoded_layer_4 = Dense(700, activation='tanh')(encoded_layer_3)\n",
    "    encoded = Dense(encoding_dim, activation='tanh')(encoded_layer_4)\n",
    "    decoded_layer_2 = Dense(700, activation='tanh')(encoded)\n",
    "    decoded_layer_3 = Dense(1000, activation='tanh')(decoded_layer_2)\n",
    "    decoded_layer_4 = Dense(1500, activation='tanh')(decoded_layer_3)\n",
    "    decoded = Dense(2000, activation='tanh')(decoded_layer_4)\n",
    "    encoder=Model(input_img,encoded)\n",
    "    autoencoder=Model(input_img,decoded)\n",
    "    return encoder,encoding_dim,autoencoder\n",
    "\n",
    "def assign_func(z_test,z_ic):\n",
    "    dst = np.array(np.sum(((z_test-z_ic[0]) **2),axis=1))\n",
    "    for index in range(1, z_ic.shape[0]):\n",
    "        col = np.sum(((z_test- z_ic[index])**2), axis=1)\n",
    "        dst = np.vstack((dst, col))\n",
    "    dst = dst.T      \n",
    "    \n",
    "    mean_dst = np.mean(dst, axis=1)\n",
    "    q = np.maximum(0.0, np.tile(mean_dst, (dst.shape[1], 1)).T - dst)\n",
    "    num_centers = q.shape[1]\n",
    "    weight = 1.0 / (q.sum(axis=0) + 1e-9)\n",
    "    weight *= num_centers / weight.sum()\n",
    "    q = (q ** 2.0) * weight\n",
    "    q = (q.T / (q.sum(axis=1) + B.epsilon())).T\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80373db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/itu-cvl/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "loading trained model\n"
     ]
    }
   ],
   "source": [
    "encoder,encoding_dim,autoencoder=arch()\n",
    "print('loading trained model')\n",
    "encoder.load_weights('../../../checkpoints/chk_mnist/sdsc_mnist40k.ckpt')\n",
    "z_test=encoder.predict(x_train)\n",
    "ic= np.load('../../../checkpoints/chk_mnist/sdsc_icmnist40k.npy')\n",
    "assignment = assign_func(z_test, ic)\n",
    "predictedlabels = np.argmax(assignment, axis=1)\n",
    "rectified_label, acc=rectify_label(predictedlabels, y)\n",
    "nmi=sklearn.metrics.normalized_mutual_info_score(y, predictedlabels)\n",
    "f1=sklearn.metrics.f1_score(y, rectified_label,average='micro')\n",
    "prec=sklearn.metrics.precision_score(y, rectified_label,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8d1570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96775 0.9160532969450231 0.96775 0.9679329214317739\n"
     ]
    }
   ],
   "source": [
    "print(acc,nmi,f1,prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914e7bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9701333333333333 0.9215328881744033 0.9701333333333333 0.9702494630546102\n"
     ]
    }
   ],
   "source": [
    "x=np.load('../../../datasets/MNIST/d_mnist70k.npy')\n",
    "x=x.reshape(x.shape[0]*x.shape[1],2000)\n",
    "y=np.load('../../../datasets/MNIST/l_mnist70k.npy')\n",
    "y=y.reshape(y.shape[0]*y.shape[1],)\n",
    "x_test=x[40000:,:]\n",
    "y_test=y[40000:,]\n",
    "z_test=encoder.predict(x_test)\n",
    "assignment = assign_func(z_test, ic)\n",
    "predictedlabels = np.argmax(assignment, axis=1)\n",
    "rectified_label, acc=rectify_label(predictedlabels, y_test)\n",
    "nmi=sklearn.metrics.normalized_mutual_info_score(y_test, predictedlabels)\n",
    "f1=sklearn.metrics.f1_score(y_test, rectified_label,average='micro')\n",
    "prec=sklearn.metrics.precision_score(y_test, rectified_label,average='weighted')\n",
    "print(acc,nmi,f1,prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2d9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
